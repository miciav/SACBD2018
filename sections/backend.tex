\subsection{Data wrangling: run time}

%We have introduced the platform components in the previous section, in this section we focus on the services of the corporate section. In the most general case, these components are physically distant from the platform components and interact with them via APIs. 
%In the early stages of pilot development, we do not rule out the possibility that the interaction between the components of the two logic areas can be carried out manually by means of one or more human users.  

%The Big Data Runtime (BDR) component's role in our solution is to perform batch data cleaning, integration, enrichment and data transformation at scale. 
%The objective of the pipeline is to eventually expose the data in a central database for performing analytics.

The Processing component of the EW-Shopp platform is responsible for processing Big Data. Our solution relies on a Docker Container Orchestration system (e.g., Kubernetes\footnote{\url{https://kubernetes.io/}}, Rancher\footnote{\url{https://rancher.com/}}, etc.) for the enactment of the data workflows that are defined in the System Orchestration component and converted to a stack configuration. Container Orchestration based data processing allows for deployment of data workflows on heterogeneous hardware infrastructures that may be found in organisations. It allows for flexible deployments and seamless horizontal scaling if set up right. Our approach to implementing data workflows takes advantage of a set of fairly standard tools and services that need to be deployed on the cluster of machines that will be used for processing. The machines are connected via an Ethernet fabric and mount a shared filesystem (e.g., GlusterFS\footnote{\url{https://www.gluster.org/}}). Each machine runs the Docker engine, so that Docker containers can be deployed. The particular virtual hardware allocated to each of the Docker container that implement the processing pipeline depends on the load and is configurable. The deployment of Docker containers is controlled at run time by the container orchestration system. The execution procedure for the data workflow involves then the following aspects:
\begin{description}
    \item[Deployment of the cluster configuration] - the cluster configuration contains parameters related to the actions that are to be fulfilled by the data workflow, as well as any additional hardware constraints for each step. These parameters are fed to a set of pre-built Docker images, which implement the steps library (custom steps can be included as needed). Each image is continuously running a bash script, which invokes necessary functionalities, logs results/errors and ensures the workflow is running smoothly. The Container Orchestration engine chooses an optimal deployment on the given cluster and starts the necessary set of Docker containers (based on the images) that will execute the workflow. 
    %in order to handle the scale of data, the Big Data Runtime component has to be able to distribute the individual workflow steps over a cluster of virtual machines (VMs) or containers. The cluster can be deployed on the premise of the EW-Shopp platform user to maintain the confidentiality of their data. The cluster also needs to provide access to the input data to all members of the cluster, for example by deploying data are on a reachable network location or on a mounted shared file system.
    \item[Execution of the data workflow] - each of the steps of the workflow is set up to work independently and during the execution phase, the containers can be scaled up or down on demand. The individual steps pass results between each other through the file system, which is structured in a specific way. In essence, each container is given an input, work and output folder and the cluster configuration reflects those settings. Race conditions between individual containers that implement the steps are solved using low-level file operations.
    %The Big Data Runtime executes a pipeline consisting of the steps specified by the users. To provide an overview of the execution and feedback on the results, the workflow is defined and displayed through a user interface. The data workflows need to be reused when new data comes in the shared input data steps. The data workflow steps need to be able to be scaled up and down, automatically, or semi-automatically, according to their load.
    \item[Data Storage] - The results of the data workflow can be stored on disk or (more typically) imported to an Enrichment Database (reachable through the network of the cluster). The database serves as a data lake and can be used, for example, to slice and dice the data to obtain a subset needed for analytics or machine learning jobs. 
    %The Big Data Runtime deploys the output of the data workflow in a central data storage solution, which is accessible through the network, or deployed on the same cluster. 
\end{description} 
The tools and services for providing analytics and reporting functions consume the data produced by the data workflow executed by the Big Data Runtime (BDR) through the central database.