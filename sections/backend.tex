\subsection{Corporate Services}\label{sec:bdr}

%We have introduced the platform components in the previous section, in this section we focus on the services of the corporate section. In the most general case, these components are physically distant from the platform components and interact with them via APIs. 
%In the early stages of pilot development, we do not rule out the possibility that the interaction between the components of the two logic areas can be carried out manually by means of one or more human users.  

%The Big Data Runtime (BDR) component's role in our solution is to perform batch data cleaning, integration, enrichment and data transformation at scale. 
%The objective of the pipeline is to eventually expose the data in a central database for performing analytics.

The Corporate services of the EW-Shopp solution are mainly responsible for the orchestration and execution of wrangling operations at scale. 
In particular, based on the user-defined data wrangling pipeline (cleaning, transformation, graph mapping, enrichment, and extension) declared on the sampled data, a so-called transformation model is created and transmitted to the corporate services in order to be applied to the full data set. The transformation model is generated by the Data Wrangler component and packaged (compiled) in a self-contained executable Java archive (JAR) directly from the user interface of Grafterizer. The self-contained executables are compiled at runtime by using the fact that the Grafterizer data wrangling pipelines are translated to Clojure\footnote{\url{https://clojure.org/}} code, which can be executed on the Java Virtual Machine. This JAR is then used as input to the main step of a larger process that is referred to as Data Flow.
% Once there, the model is compiled into a self-contained executable Java archive (JAR) and executed as the main step of a larger process that is referred to as Data Flow.  
Data Flows are an extension of the data wrangling pipeline with pre- and post-processing actions. 
To serve as an example, a data flow coming from a business case of EW-Shopp is described below:
\begin{enumerate}
    \item Decompress data - large data set is delivered as a set of archives contain CSV files, which are stored in a shared file system of our private cloud.
    \item Split data in chunks for faster processing and horizontal scalability of inputs.
    \item Execute the data wrangling pipeline - Clean up, re-format, transform, map to ontology, and enrich the split data. 
    \item Import the resulting data set into the Enrichment database
\end{enumerate}



The System Orchestration component provides a high-level interface to handle and monitor data flows, whereas the Processing component is in charge of carrying them out. From the technological point of view, data flows are compiled into a chain of Docker\footnote{https://www.docker.com/} containers that are in turn deployed and run through a Container Orchestration system (e.g., Kubernetes\footnote{\url{https://kubernetes.io/}}, Rancher\footnote{\url{https://rancher.com/}}) on a cluster of machines connected via an Ethernet fabric and mounting a shared filesystem (e.g., GlusterFS\footnote{\url{https://www.gluster.org/}}).
The implementation of a container-based solution has several benefits; for instance, it allows to decouple the data flow deployment from the particular stakeholder's hardware infrastructure also working in heterogeneous distributed environments. Furthermore, it guarantees flexible deployments, better resource utilization, and seamless horizontal scalability.  

 
%Each machine runs the Docker engine, so that Docker containers can be deployed. The particular virtual hardware allocated to each of the Docker container that implement the processing pipeline depends on the load and is configurable. The deployment of Docker containers is controlled at run time by the container orchestration system. 
Finally, for the sake of completeness, the following non-secondary aspects of the execution of a data flow are presented:
\begin{description}
    \item[Deployment of the flow configuration] - the flow configuration contains the context information to be fed to each step that composes the data flow, as well as any additional hardware constraints for each step. These parameters are passed to the containers in the chain they are intended for. Notice that an extensible library implements the most common (parametrized) actions to be used in the definition of a data flow. Moreover, each action implements a common interface to guarantee both composability and observability. 
    
    %in order to handle the scale of data, the Big Data Runtime component has to be able to distribute the individual workflow steps over a cluster of virtual machines (VMs) or containers. The cluster can be deployed on the premise of the EW-Shopp platform user to maintain the confidentiality of their data. The cluster also needs to provide access to the input data to all members of the cluster, for example by deploying data are on a reachable network location or on a mounted shared file system.
    \item[Execution of the data flow] - The Processing component establishes a suitable deployment in terms of machines and resources to be used and starts the appropriate number of containers for each step of the flow.  Each of the steps consists of a set of containers that work independently and in parallel and can be scaled up or down on demand if required. The communication between two consecutive steps of the chain, that is, the handover of the partial results, occur through writing and reading from a file system in a specific way, defined in the context information.
%a specific way: each container is given an input, work and output folder and the cluster configuration reflects those settings. %Race conditions between individual containers that implement the steps are solved using low-level file operations.
    \item[Data Storage] - The results of the data flow can be stored on disk or (more typically) imported to an Enrichment Database. The database serves as a single source of truth to be used, for example, to slice and dice the data to obtain a subset needed for analytics or machine learning jobs. 
\end{description} 
%The tools and services for providing analytics and reporting functions consume the data produced by the data workflow executed by the Big Data Runtime (BDR) through the central database.