\section{Introduction}



Over the past years Big Data has been receiving ever more attention both in academia and industry. 
%For some years now we have been witnessing the flourishing of Big Data in Academia but especially in Industry. 
This can be partly explained by the promise of Big Data advocates to create value (estimates indicate yearly earnings in the order of 200 billion dollars by 2020~\cite{idc_analytics}) delivering more accurate and efficient data-driven decision-making processes via analytics. 
The term \textit{Data Analytics}, however, is vague and too often used to define a class of heterogeneous activities such as \textit{data wrangling}~\cite{furche2016data} (that includes preparation processes as cleaning, linking, enrichment/extension), training and application of analytic models, up to business intelligence and visualization. 
In this paper, we focus on data wrangling on tabular data, as the correct definition of data preparation processes is a painful and time-consuming (up to the 80\% of total time~\cite{lohr2014big} for performing analytics) hurdle in the implementation of a value-added data driven pipelines.
In particular, we describe a software solution (part of a larger ecosystem) tailored to perform data wrangling and generation of linked data, which is able to operate on massive data sets with a special support provided to the integration of weather and events data. This platform has been designed and realized following the guidelines of the Lean methodology, based on a rigorous activity of requirements elicitation that took into account both literature and best practices as well as the needs expressed by involved stakeholders. 


%One might wonder about the rationale behind creating a new Big Data solution when the market already offers many widely adopted proprietary and open-source alternatives. 
%The answer lies in the nature of the operations to carry out on the data. 
%Most available platforms, in fact, feature a general-purpose processing model designed for users familiar with programming languages and process definition, but usually inexperienced in the particular domain to which the data pertain~\cite{sukhobok2016tabular}. 
Although there are many solutions for processing Big Data, most available platforms feature a general-purpose processing model designed for users familiar with programming languages and process definition, but usually inexperienced in the particular domain to which the data pertain~\cite{sukhobok2016tabular}. For this reason, they do not provide specific and user-friendly tools for the definition of data transformation pipelines~\cite{rahm2000data} (design time), and, instead, often focus only on the management of their execution (run time). The main implication is the emergence of a two-pronged working environment consisting, on the one hand, of domain experts in charge of designing data transformations and, on the other hand, of engineers who deploy them into a production environment. This socio-occupational gap between interdependent groups pigeonholed in strictly separated roles can cause issues and delays in development and maintenance of Big Data solutions and calls for specific solutions to be bridged.  

The proposed platform for designing and executing data transformation pipelines (from the cleaning phase to linking and publication) is being realized in the context of EW-Shopp. This is a H2020 EU project aiming at supporting the e-commerce, retail and marketing industries in improving their efficiency and competitiveness through enabling them to perform predictive and prescriptive analytics over integrated, enriched, and extended large data sets using open and flexible solutions. 
This paper describes the architecture of a subsystem of the EW-Shopp\footnote{\url{http://www.ew-shopp.eu/}} integrated platform aimed at facilitating and enhancing data transformation, integration, and extension processes with focus on large scale integration of weather and event information.

The paper is structured as follows. 
In Section~\ref{sec:approach} the main design principles that have driven the definition of the architecture are presented whereas the components and workflow are discussed in Section~\ref{sec:architecture}. %Section~\ref{sec:evaluation} illustrates some preliminary experiments with a large data set.
Finally, Section~\ref{sec:conclusions} concludes the document. 








