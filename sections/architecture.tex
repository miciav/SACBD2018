\section{Data Wrangling at Scale}\label{sec:architecture}
As mentioned above, in the designed architecture a logical separation between platform and corporate services has been imposed. This structure is conceived to make the architecture flexible and adaptable to different deployment scenarios. 
In what follows, the main components and interaction of the platform are presented. 

Figure~\ref{fig:wrangler} shows a component diagram of our wrangling solution with component interactions and information flows. 
The image presents the most general case, where the data set to be transformed is genuine Big Data and, for this reason, it is managed by the corporate services. 
In this scenario, components from all logical areas are involved; we have on the left a corporate component called \textit{Sampler} , the \textit{Summarizer} and the \textit{Engine} that are the components that are entrusted of creating the (reduced) working data set to be used to define the transformations, of providing a set of suggestions for the table annotation process based on summaries of existing knowledge bases or previous annotations, and finally, of interacting with the \textit{Big Data Runtime} to carry out on a large scale the transformations defined by the user, respectively. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/Wrangler.pdf}
    \caption{Data Wrangler's components and interactions}
    \label{fig:wrangler} 
\end{figure}  
The definition of the transformation pipeline, instead, is carried out using the platform services, depicted on the right side of the Figure. In particular, the user is supported in this task by a graphical manipulation interface, which in turn interacts with a \textit{Transformation and Enrichment} back-end service. This service has the twofold duty of executing the pipeline on the reduced data set, thus allowing the platform to be used as a standalone solution, and of interacting with the corporate services. In both cases, core data services support linking and extension capabilities. 

The architecture and components interactions within the Big Data Runtime are depicted in Figure~\ref{fig:big_data_runtime}. The \textit{System Orchestration} sub-component is used to define the high-level data flows that will be executed by \textit{Processing} tool. 
Such Data flows include the data wrangling pipelines but they may also incorporate pre- and post-processing steps, e.g, automatic obtaining of data, re-formatting, import to a data warehouse (enrichment database). The end result is a data flow that can be deployed as a Function as a Service (FaaS) computing service over a managed cluster of resources, which allows for easy integration with business processes and heterogeneous infrastructures (see Section~\ref{sec:bdr}).

For sake of clarity a high-level workflow is reported:
\begin{enumerate}
    \item A reduced data set (possibly complemented by profiling information) is created and passed to the \textit{Transformation and Enrichment} component.
    \item The user operates the \textit{Data Manipulation UI}, which also interacts with the Core Data services and \textit{Summarizer} to perform schema and entity linking, to extend the data with weather and events information.
    \item The application generates a self-contained machine-runnable pipeline of the user's operations which is eventually executed on the original data set by the \textit{Big Data Runtime}. 
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[ width=0.9\columnwidth]{figs/SACBD2018-Big-data-processing-engine.png}
    \caption{Big Data Runtime components interaction}
    \label{fig:big_data_runtime} 
\end{figure}  

The next two subsections detail the components and interactions of the proposed solution, highlighting which components are involved at design time and which at run time. 


\subsection{Platform services}

The Data Wrangler is a composite component built upon the DataGraft platform~\cite{roman2016datagraft}, extended with semantic enrichment and Big Data processing capabilities. DataGraft provides tools for cleaning and transformation of tabular data into RDF and graph generation mappings.
%featuring a composite architecture that follows the service-oriented architecture mandates. 
DataGraft comes with an interactive web application, named Grafterizer~\cite{sukhobok2016tabular}, which serves as a graphical interface helping platform users to transform data from a tabular format into a graph format. The transformation supports both cleaning and graph mapping steps. Transformation steps on rows (add, drop, filter, duplicate detection etc.), columns (add, drop, rename, merge etc.) and entire data set (sort, aggregate etc.) are provided together with visualization of the result after each step. The mapping to ontologies or vocabularies is performed on the cleaned-up data. 
%Moreover, the DataGraft is responsible for user-facing miscellaneous tasks such as user management, managing user assets (i.e., Enrichment Database endpoints, queries, transformation models) and enabling easy access to data.  

As mentioned, the EW-Shopp platform aims to support scalable data enrichment. Therefore, DataGraft and the transformation tool, Grafterizer, incorporate two sub-components called ASIA and ABSTAT. The components provide functionalities for the semantic enrichment of data tables and profiling of knowledge graphs, respectively. ASIA is meant to aid users in integrating business data and can be used to map the data schema to shared vocabularies/ontologies, or link data values to shared systems of identifiers, which enables the extraction of additional data from third-party sources and their fusion into the original tabular data. Schema-level and instance-level links are created by ASIA as annotations for the table. ABSTAT~\cite{palmonari2015abstat} is a tool to profile knowledge graphs represented in RDF based on linked data summarization mechanisms. The profiles extracted by ABSTAT describe the content of knowledge graphs, using abstraction (schema-level patterns) and statistics. Such profiles are exploited by ASIA to provide the user with suggestions in schema linking activity. 

\input{sections/backend.tex} 