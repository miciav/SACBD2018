\section{Data Wrangling at Scale}\label{sec:architecture}
As mentioned above, in the designed architecture a logical separation between  platform and corporate services has been imposed. This structure is conceived to make the architecture flexible and adaptable to different deployment scenarios. 
In what follows, the main components and interaction of the platform are presented. 

Figure~\ref{fig:wrangler} shows a component diagram of our wrangling solution with component interactions and information flows. 
The image presents the most general case, where the data set to be transformed is genuine Big Data and, for this reason it is managed by the corporate services. 
In this scenario, components from all logical areas are involved; we have on the left a corporate component called \textit{Sampler} , the \textit{Summarizer} and the \textit{Engine} that are the components that are entrusted of creating the (reduced) working data set to be used to define the transformations, of providing a set of suggestions for the table annotation process based on summaries of existing knowledge bases or previous annotations, and finally, of interacting with the \textit{Big Data Runtime} to carry out on a large scale the transformations defined by the user, respectively. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/Wrangler.pdf}
    \caption{Data Wrangler's components and interactions}
    \label{fig:wrangler} 
\end{figure}  
The definition of the transformation pipeline, instead, is carried out using the platform services, depicted in the right side of the Figure. In particular, the user is supported in this task by a graphical manipulation interface, which in turn interacts with a \textit{Transformation and Enrichment} back-end service. This service has the twofold duty of executing the pipeline on the reduced data set, thus allowing the platform to be used as a standalone solution, and of interacting with the corporate services. In both cases, core data services support linking and extension capabilities. 

The Big Data Runtime and processing interaction is depicted on Figure~\ref{fig:big_data_runtime}. At design time, the System Orchestration sub-component is used to define the high-level data flows that will be executed by the run time, which include the data wrangling pipelines. The high-level steps of the flows may also incorporate pre- and post-processing steps, e.g, automatic obtaining of data, re-formatting, import to data warehouse (enrichment database). The end result is a data flow that can be deployed as a Function as a Service (FaaS) computing service over a managed cluster of resources, which allows for easy integration with business processes and heterogeneous infrastructures (see Section~\ref{sec:bdr}).



For sake of clarity a high-level workflow is reported:
\begin{enumerate}
    \item A reduced data set (possibly complemented by profiling information) is created and passed to the \textit{Transformation and Enrichment} component.
    \item The user operates the \textit{Data Manipulation UI}, which also interacts with the Core Data services and \textit{Summarizer} to perform schema and entity linking, to extend the data with weather and events information.
    \item The application generates a self-contained machine-runnable pipeline of the user's operations which is eventually executed on the original data set by the \textit{Big Data Runtime}. 
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[ width=0.9\columnwidth]{figs/SACBD2018-Big-data-processing-engine.png}
    \caption{Big Data Runtime components interaction}
    \label{fig:big_data_runtime} 
\end{figure}  

The next two subsections detail the components and interactions of the proposed solution, highlighting which components are involved at design time and which at run time. 


\subsection{Data wrangling: design time}

The Data Wrangler is a composite component built upon the DataGraft platform~\cite{roman2016datagraft}, extended with semantic enrichment and Big Data processing capabilities. DataGraft provides tools for cleaning and transformation of tabular data into RDF and graph generation mappings.
%featuring a composite architecture that follows the service-oriented architecture mandates. 
DataGraft comes with an interactive web application, named Grafterizer~\cite{sukhobok2016tabular}, which serves as user interface helping platform users to transform data from a tabular format into a graph format. The transformation supports both cleaning and graph mapping steps. Transformation steps on rows (add, drop, filter, duplicate detection etc.), columns (add, drop, rename, merge etc.) and entire data set (sort, aggregate etc.) are provided together with visualization of the result after each step. The mapping to ontologies or vocabularies is performed on the cleaned-up data. 
%Moreover, the DataGraft is responsible for user-facing miscellaneous tasks such as user management, managing user assets (i.e., Enrichment Database endpoints, queries, transformation models) and enabling easy access to data.  

As mentioned, the EW-Shopp platform aims to support scalable data enrichment. Therefore, DataGraft and the transformation tool, Grafterizer, incorporate two sub-components called ASIA and ABSTAT. The components provide functionalities for the semantic enrichment of data tables and profiling of knowledge graphs, respectively. ASIA is meant to aid users in integrating business data and can be used to map the data schema to shared vocabularies/ontologies, or link data values to shared systems of identifiers, which enables the extraction of additional data from third-party sources and their fusion into the original tabular data. Schema-level and instance-level links are created by ASIA as annotations for the table. ABSTAT~\cite{palmonari2015abstat} is a tool to profile knowledge graphs represented in RDF based on linked data summarization mechanisms. The profiles extracted by ABSTAT describe the content of knowledge graphs, using abstraction (schema-level patterns) and statistics. Such profiles are exploited by ASIA to provide the user with suggestions in schema linking activity. 

Based on the user-defined data cleaning, transformation, graph mapping, and enrichment steps declared on the sampled data, Grafterizer is able to generate a self-contained executable Java archive (JAR), which contains these aspects. This executable is then used as a step of the data workflow, which consumes and processes the full dataset. The System Orchestration component provides a high-level interface to declare and monitor such data flows based on an extensible library of functionalities. One simplified example of such a workflow coming from a practical scenario in EW-Shopp is as follows:
\begin{enumerate}
    \item Decompress data - input data is delivered as a set of archives, which are stored in a folder on a shared file system and contain CSV files to be transformed to a graph format
    %\item Split data in chunks (if necessary) - library step that lets users split files in smaller chunks for faster processing and horizontal scalability of inputs
    \item Clean up, re-format, transform, map to ontology, and enrich the split data - implemented by a workflow step that consumes the executable JAR file that is produced by Grafterizer
    \item Import data to the Enrichment database
\end{enumerate}

The data workflow definition by the System Orchestration component is converted to a stack configuration, which is used by the Processing component at runtime.
%Nikolay: too much low-level detail 
%Users can create schema-level annotations through the ASIA interface, by validating ASIA suggestions about classes and properties to be used. If a user specifies a different class (or property), ASIA suggests classes (or properties) that syntactically match the user input implementing the autocomplete functionality. Otherwise, the instance-level annotations are expected to be created by ASIA semi-automatically.

%Nikolay: too much low-level detail 
%As a matter of fact, summaries record rich statistics about the usage of vocabularies/ontologies in data sets. Thus, they can provide types and properties that match a string, for example, the header of a column in a table to be published. In addition, statistics provide valuable information to algorithms aimed at suggesting the best properties and types to use when transforming a tabular data into RDF.  
\input{sections/backend.tex} \label{sec:bdr};